# CLIP-CC Bench: Evaluating Paragraph-Level Video Descriptions in Video–Language Models

A comprehensive evaluation framework that uses state-of-the-art text embedding models to measure the quality of video descriptions generated by vision-language models (VLMs). The framework computes semantic similarity between model predictions and reference descriptions at both document-level and sentence-level granularity.

## Overview

CLIP-CC Bench is a text embedding-based evaluation framework that:
- Evaluates 17 vision-language models (VLMs) using an ensemble of 5 state-of-the-art text embedding models
- Measures description quality through multiple similarity metrics (cosine similarity, fine-grained precision/recall/F1)
- Supports both coarse-grained (full paragraph) and fine-grained (sentence-level) evaluation
- Provides isolated implementations for each embedding model ensuring reproducibility
- Ranks VLMs based on aggregated performance across all embedding model judges

**Important Note**: This is an evaluation framework for measuring VLM performance, not a video captioning system. It compares VLM-generated descriptions against reference descriptions using semantic similarity.

## Project Structure

```
clip-cc-bench/
├── src/
│   ├── configs/                      # Embedding model configurations (YAML)
│   │   ├── nv-embed.yaml            # NV-Embed-v2 configuration
│   │   ├── kalm.yaml                # KaLM-Embedding-Gemma3-12B-2511
│   │   ├── nemo.yaml                # llama-embed-nemotron-8b
│   │   ├── gte.yaml                 # gte-Qwen2-7B-instruct
│   │   └── qwen.yaml                # Qwen3-Embedding-8B
│   ├── scripts/                      # Evaluation and ranking scripts
│   │   ├── run_nv_embed_evaluation.py
│   │   ├── run_kalm_evaluation.py
│   │   ├── run_nemo_evaluation.py
│   │   ├── run_gte_evaluation.py
│   │   ├── run_qwen_evaluation.py
│   │   └── rank_vlms.py             # VLM ranking algorithm
│   └── utils/                        # Model implementations and utilities
│       ├── nv_embed_model.py        # Model-specific implementations
│       ├── kalm_model.py
│       ├── nemo_model.py
│       ├── gte_model.py
│       ├── qwen_model.py
│       ├── result_manager.py        # Results aggregation system
│       ├── base_types.py            # Shared data structures
│       ├── text_chunking.py         # Sentence tokenization
│       └── paths.py                 # Path management
├── embedding_models/                 # Model weights directory
│   ├── NV-Embed-v2/                 # See embedding_models/README.md
│   ├── KaLM-Embedding-Gemma3-12B-2511/
│   ├── llama-embed-nemotron-8b/
│   ├── gte-Qwen2-7B-instruct/
│   └── Qwen3-Embedding-8B/
├── venv_configs/                     # Virtual environment activation scripts
│   ├── activate_nv-embed_env.sh     # See venv_configs/README.md
│   ├── activate_kalm_env.sh
│   ├── activate_nemo_env.sh
│   ├── activate_gte_env.sh
│   └── activate_qwen_env.sh
├── data/
│   ├── ground_truth/
│   │   └── clip_cc_dataset.json     # Reference descriptions
│   └── predictions/                  # VLM-generated descriptions
│       ├── internvl.json
│       ├── llava_next_video.json
│       └── ... (17 VLM models)
├── results/                          # Evaluation outputs
│   ├── embedding_models/
│   │   ├── logs/                    # Per-embedding model execution logs
│   │   ├── aggregated_results/      # Summary statistics per VLM
│   │   ├── individual_results/      # Per-video, per-VLM detailed results
│   │   │   ├── json/
│   │   │   └── csv/
│   │   └── aggregated_results.csv   # Cross-model summary table
│   └── ranking/                     # VLM ranking results
│       ├── vlm_overall_ranking.csv
│       └── vlm_per_judge_metrics.csv
├── run_all_evaluations.sh           # Master evaluation script
└── README.md
```

## Embedding Models

The framework uses an ensemble of 5 state-of-the-art text embedding models as judges:

| Embedding Model | Parameters | Embedding Dim | Max Length | Implementation | Status |
|-----------------|-----------|---------------|------------|----------------|---------|
| **NV-Embed-v2** (NVIDIA) | - | 4,096 | 32,768 | Custom local model | ✅ Verified |
| **KaLM-Embedding-Gemma3-12B-2511** (PGFoundation) | 12B | - | - | SentenceTransformer | ✅ Verified |
| **llama-embed-nemotron-8b** (NVIDIA) | 8B | 4,096 | 4,096 | SentenceTransformer | ✅ Verified |
| **gte-Qwen2-7B-instruct** (Alibaba) | 7B | 3,584 | 32,768 | SentenceTransformer | ✅ Verified |
| **Qwen3-Embedding-8B** (Qwen) | 8B | Variable | 32,768 | SentenceTransformer | ✅ Verified |

**Implementation Verification**: All models use official/recommended implementations from their respective HuggingFace model cards, ensuring consistency with published results.

## Video Language Models Evaluated

The framework evaluates predictions from **17 vision-language models**:

- **InternVL** (internvl)
- **LLaVA-NeXT-Video** (llava_next_video)
- **LLaVA-OneVision** (llava_one_vision)
- **LongVA** (longva)
- **LongVU** (longvu)
- **MiniCPM** (minicpm)
- **mPLUG** (mplug)
- **Oryx** (oryx)
- **Qwen2.5-32B** (Qwen2.5-32B)
- **Qwen2.5-72B** (Qwen2.5-72B)
- **ShareGPT4** (sharegpt4)
- **TimeChat** (timechat)
- **TS-LLaVA** (ts_llava)
- **Video-XL** (video_xl)
- **VideoChatFlash** (videochatflash)
- **VideoLLaMA3** (videollama3)
- **ViLaMP** (vilamp)

## Setup

### Prerequisites

- Python 3.9+
- CUDA-capable GPU (24GB+ VRAM recommended for large embedding models)
- ~70GB disk space for all embedding models
- PyTorch 2.0+
- transformers >= 4.42.0
- sentence-transformers >= 2.7.0

### Installation

**1. Clone the repository**
```bash
git clone https://github.com/YOUR_USERNAME/clip-cc-bench.git
cd clip-cc-bench
```

**2. Setup virtual environments**

Each embedding model requires an isolated virtual environment with specific dependencies. See `venv_configs/README.md` for detailed setup instructions:

```bash
# Setup all 5 virtual environments
# Follow instructions in venv_configs/README.md
```

Quick activation reference:
```bash
# Activate specific environment
source venv_configs/activate_nv-embed_env.sh
source venv_configs/activate_kalm_env.sh
source venv_configs/activate_nemo_env.sh
source venv_configs/activate_gte_env.sh
source venv_configs/activate_qwen_env.sh

# Deactivate when done
deactivate
```

**3. Download embedding models**

Download model weights from HuggingFace to the `embedding_models/` directory. See `embedding_models/README.md` for complete download instructions and storage requirements (~70GB total).

**4. Prepare evaluation data**

Organize your data files:
```bash
# Reference descriptions (ground truth)
data/ground_truth/clip_cc_dataset.json

# VLM-generated predictions
data/predictions/{vlm_name}.json
```

## Reproducing Results

### Option 1: Run All Evaluations (Recommended)

The master script runs all 5 embedding model evaluations sequentially and computes final VLM rankings:

```bash
./run_all_evaluations.sh
```

This script will:
1. Activate each embedding model's virtual environment
2. Run evaluation for all 17 VLMs using that embedding model
3. Generate individual results, aggregated statistics, and logs
4. Move to the next embedding model
5. Compute final VLM rankings across all embedding models
6. Save comprehensive results to `results/` directory

**Output locations:**
- Logs: `results/embedding_models/logs/{embedding_model_name}/`
- Individual results: `results/embedding_models/individual_results/json/{embedding_model_name}/`
- Aggregated statistics: `results/embedding_models/aggregated_results/{embedding_model_name}/`
- Cross-model summary: `results/embedding_models/aggregated_results/aggregated_results.csv`
- VLM rankings: `results/ranking/vlm_overall_ranking.csv`

### Option 2: Run Individual Embedding Models

Evaluate using a specific embedding model:

```bash
# Activate the target environment
source venv_configs/activate_nv-embed_env.sh

# Run evaluation with config file
python src/scripts/run_nv_embed_evaluation.py --config src/configs/nv-embed.yaml

# Deactivate when done
deactivate
```

Repeat for other embedding models:
```bash
# KaLM
source venv_configs/activate_kalm_env.sh
python src/scripts/run_kalm_evaluation.py --config src/configs/kalm.yaml
deactivate

# Nemotron
source venv_configs/activate_nemo_env.sh
python src/scripts/run_nemo_evaluation.py --config src/configs/nemo.yaml
deactivate

# GTE
source venv_configs/activate_gte_env.sh
python src/scripts/run_gte_evaluation.py --config src/configs/gte.yaml
deactivate

# Qwen3
source venv_configs/activate_qwen_env.sh
python src/scripts/run_qwen_evaluation.py --config src/configs/qwen.yaml
deactivate
```

### Option 3: Evaluate Specific VLMs Only

To evaluate a subset of VLMs (useful for testing):

```bash
source venv_configs/activate_nv-embed_env.sh

python src/scripts/run_nv_embed_evaluation.py \
  --config src/configs/nv-embed.yaml \
  --models internvl llava_one_vision longvu

deactivate
```

### Generate VLM Rankings

After running all embedding model evaluations, generate the final rankings:

```bash
python src/scripts/rank_vlms.py
```

This creates:
- `results/ranking/vlm_overall_ranking.csv` - Final VLM rankings with mean scores across all judges
- `results/ranking/vlm_per_judge_metrics.csv` - Detailed per-embedding-model metrics

## Understanding Results

### Result Files Structure

```
results/embedding_models/
├── logs/
│   ├── NV-Embed-v2/
│   │   └── nv_embed_evaluation_YYYYMMDD_HHMMSS.log
│   ├── KaLM-Embedding-Gemma3-12B-2511/
│   ├── llama-embed-nemotron-8b/
│   ├── gte-Qwen2-7B-instruct/
│   └── Qwen3-Embedding-8B/
├── aggregated_results/
│   ├── NV-Embed-v2/
│   │   ├── internvl.json
│   │   ├── llava_next_video.json
│   │   └── ... (one per VLM)
│   ├── KaLM-Embedding-Gemma3-12B-2511/
│   ├── ... (per embedding model)
│   └── aggregated_results.csv        # Cross-model summary
├── individual_results/
│   ├── json/
│   │   ├── NV-Embed-v2/
│   │   │   ├── internvl.json         # All videos for this VLM
│   │   │   └── ...
│   │   └── ... (per embedding model)
│   └── csv/
│       └── ... (CSV versions)

results/ranking/
├── vlm_overall_ranking.csv           # Final rankings
└── vlm_per_judge_metrics.csv         # Per-judge details
```

### Individual Result Format

Each individual result file contains per-video evaluation data:

```json
{
  "001": {
    "reference": "A person walks across a bridge while the sun sets...",
    "candidate": "A man walking on a bridge during sunset...",
    "reference_length": 150,
    "candidate_length": 48
  },
  "002": {
    "reference": "...",
    "candidate": "..."
  }
}
```

### Aggregated Result Format

Aggregated files contain summary statistics for each VLM:

```json
{
  "embedding_model_name": "NV-Embed-v2",
  "model_name": "internvl",
  "total_evaluations": 1000,
  "successful_evaluations": 998,
  "error_rate": 0.002,
  "coarse_grained_stats": {
    "mean": 0.7845,
    "std": 0.0823,
    "min": 0.4521,
    "max": 0.9876,
    "median": 0.7912
  },
  "fine_grained_stats": {
    "precision": {"mean": 0.7621, "std": 0.0912},
    "recall": {"mean": 0.7834, "std": 0.0845},
    "f1": {"mean": 0.7726, "std": 0.0876}
  },
  "hybrid_stats": {
    "hm_cf": {"mean": 0.7785, "std": 0.0849}
  },
  "timestamp": "2025-11-20T12:34:56"
}
```

### Cross-Model Summary Table

The `aggregated_results.csv` file presents a hierarchical view:

| VLM | NV-Embed-v2_coarse | NV-Embed-v2_fine_f1 | ... | KaLM_coarse | ... |
|-----|-------------------|---------------------|-----|-------------|-----|
| internvl | 0.78±0.08 | 0.77±0.09 | ... | 0.81±0.07 | ... |
| llava_one_vision | 0.75±0.09 | 0.73±0.10 | ... | 0.79±0.08 | ... |

Format: `mean±std` for each metric

### VLM Ranking Files

**Overall Ranking (`vlm_overall_ranking.csv`):**
```csv
rank,vlm_name,mean_coarse,mean_fine_f1,mean_harmonic_mean_cf,overall_score
1,internvl,0.7845,0.7726,0.7785,0.7785
2,llava_one_vision,0.7534,0.7321,0.7427,0.7427
...
```

**Per-Judge Metrics (`vlm_per_judge_metrics.csv`):**
```csv
vlm_name,embedding_model,coarse_mean,fine_f1_mean,harmonic_mean_cf
internvl,NV-Embed-v2,0.7845,0.7726,0.7785
internvl,KaLM-Embedding-Gemma3-12B-2511,0.8123,0.7956,0.8039
...
```

## Evaluation Metrics

The framework computes multiple similarity metrics at different granularities:

### Coarse-Grained Metrics (Document-Level)
- **Cosine Similarity**: Raw cosine similarity between full paragraph embeddings (range: [-1, 1])
- **Normalized Cosine**: Normalized version for compatibility (range: [0, 1])

### Fine-Grained Metrics (Sentence-Level)
- **Precision**: For each prediction sentence, compute similarity with all reference sentences, take maximum. Average across all prediction sentences.
- **Recall**: For each reference sentence, compute similarity with all prediction sentences, take maximum. Average across all reference sentences.
- **F1 Score**: Harmonic mean of precision and recall

### Hybrid Metrics
- **HM-CF**: Harmonic mean of coarse-grained similarity and fine-grained F1, balancing document-level and sentence-level alignment

### Implementation Details
All embedding models use raw cosine similarity (GAS-style implementation) without [0,1] normalization, matching the original GAS paper methodology. Fine-grained metrics use NLTK sentence tokenizer for text chunking.

## Configuration

Each embedding model has a YAML configuration file in `src/configs/`:

**Example: `src/configs/nv-embed.yaml`**
```yaml
embedding_model:
  name: "NV-Embed-v2"                # Model identifier
  path: "embedding_models/NV-Embed-v2"  # Path to model weights
  type: "nvembed"                    # Model type
  batch_size: 8                      # Batch size for encoding
  max_length: 32768                  # Maximum sequence length
  trust_remote_code: true            # Required for custom models

  # Fine-grained evaluation settings
  fine_grained:
    enabled: true                    # Enable sentence-level metrics
    chunking_method: "nltk"          # Sentence tokenizer

processing:
  device: "cuda:0"                   # GPU device
  clear_cache_interval: 25           # Clear GPU cache every N videos
  progress_interval: 10              # Log progress every N videos

data_paths:
  ground_truth_file: "data/ground_truth/clip_cc_dataset.json"
  predictions_dir: "data/predictions"
  results_base_dir: "results"

# Specify which VLMs to evaluate
models_to_evaluate:
  - internvl
  - llava_next_video
  - llava_one_vision
  - longva
  - longvu
  - minicpm
  - mplug
  - oryx
  - Qwen2.5-32B
  - Qwen2.5-72B
  - sharegpt4
  - timechat
  - ts_llava
  - video_xl
  - videochatflash
  - videollama3
  - vilamp

logging:
  level: "INFO"
```

You can customize:
- **batch_size**: Adjust based on GPU memory
- **max_length**: Truncate long texts (model-dependent)
- **device**: Specify GPU device (cuda:0, cuda:1, etc.)
- **models_to_evaluate**: Subset of VLMs to evaluate
- **clear_cache_interval**: Frequency of GPU memory cleanup

## Data Format

### Reference Descriptions (Ground Truth)

File: `data/ground_truth/clip_cc_dataset.json`

```json
[
  {
    "id": "001",
    "summary": "The video begins with a man wearing sunglasses and a light-colored shirt driving a car. He appears to be in a tense situation, gripping the steering wheel tightly. The scene transitions to a close-up of the same man, now with a determined expression, still holding the steering wheel..."
  },
  {
    "id": "002",
    "summary": "The video begins with a close-up of a person lying down with their eyes closed, wearing headphones..."
  }
]
```

**Format specifications:**
- List of objects
- Required fields: `"id"` (string), `"summary"` (string)
- IDs should be consistent with prediction files

### VLM Predictions

File: `data/predictions/{vlm_name}.json`

```json
{
  "001": "The video shows a man driving a car while appearing tense. He grips the steering wheel firmly and displays a determined expression throughout the scene...",
  "002": "A person is shown lying down with headphones on, eyes closed, suggesting they are listening to something..."
}
```

**Format specifications:**
- Dictionary mapping video IDs to predicted descriptions
- Keys must match ground truth IDs exactly
- Values are predicted paragraph-level descriptions

## Adding New Models

### Adding a New VLM for Evaluation

To evaluate a VLM not currently in the benchmark:

1. **Generate predictions** in the required format:
   ```json
   {
     "001": "Your VLM's predicted description for video 001...",
     "002": "Your VLM's predicted description for video 002...",
     ...
   }
   ```

2. **Save predictions** to `data/predictions/your_vlm_name.json`

3. **Update configs** to include your VLM:
   ```bash
   # Edit all 5 config files in src/configs/
   # Add "your_vlm_name" to the models_to_evaluate list
   ```

4. **Run evaluation**:
   ```bash
   ./run_all_evaluations.sh
   ```

   Or evaluate with specific embedding model:
   ```bash
   source venv_configs/activate_nv-embed_env.sh
   python src/scripts/run_nv_embed_evaluation.py \
     --config src/configs/nv-embed.yaml \
     --models your_vlm_name
   deactivate
   ```

5. **Check results** in:
   - `results/embedding_models/aggregated_results/{embedding_model}/your_vlm_name.json`
   - `results/ranking/vlm_overall_ranking.csv`

### Adding a New Embedding Model Judge

To add a new embedding model to the ensemble:

**1. Create virtual environment setup**

File: `venv_configs/activate_new_model_env.sh`
```bash
#!/bin/bash
VENV_DIR="venv_new_model"

if [ ! -d "$VENV_DIR" ]; then
    echo "Creating virtual environment: $VENV_DIR"
    python3 -m venv "$VENV_DIR"
fi

source "$VENV_DIR/bin/activate"
pip install -r venv_configs/new-model-requirements.txt
```

File: `venv_configs/new-model-requirements.txt`
```
torch>=2.0.0
transformers>=4.42.0
sentence-transformers>=2.7.0
# Add model-specific dependencies
```

**2. Create configuration file**

File: `src/configs/new_model.yaml`
```yaml
embedding_model:
  name: "New-Model-Name"
  path: "embedding_models/New-Model-Name"
  type: "custom"  # or "sentence_transformer"
  batch_size: 8
  max_length: 8192

  fine_grained:
    enabled: true
    chunking_method: "nltk"

processing:
  device: "cuda:0"
  clear_cache_interval: 25

data_paths:
  ground_truth_file: "data/ground_truth/clip_cc_dataset.json"
  predictions_dir: "data/predictions"
  results_base_dir: "results"

models_to_evaluate:
  - internvl
  # ... (list all 17 VLMs)

logging:
  level: "INFO"
```

**3. Implement model wrapper**

File: `src/utils/new_model_model.py`
```python
import torch
import numpy as np
from sentence_transformers import SentenceTransformer
from base_types import SimilarityScore
from text_chunking import chunk_text_into_sentences

class NewModelEmbedding:
    """Wrapper for New Embedding Model."""

    def __init__(self, model_path: str, device: str = "cuda:0", **kwargs):
        self.model_path = model_path
        self.device = device
        self.batch_size = kwargs.get('batch_size', 8)
        self.max_length = kwargs.get('max_length', 8192)
        self.model = None

    def load_model(self):
        """Load the embedding model."""
        self.model = SentenceTransformer(
            self.model_path,
            device=self.device,
            trust_remote_code=True
        )
        return True

    def encode_texts(self, texts, add_instruction=False):
        """Encode texts to embeddings."""
        embeddings = self.model.encode(
            texts,
            batch_size=self.batch_size,
            convert_to_numpy=True,
            normalize_embeddings=True
        )
        return embeddings

    def compute_similarity(self, ground_truth_text, prediction_text):
        """Compute similarity between ground truth and prediction."""
        # Coarse-grained: full text
        gt_embedding = self.encode_texts([ground_truth_text])
        pred_embedding = self.encode_texts([prediction_text])
        cosine_sim = float(np.dot(gt_embedding[0], pred_embedding[0]))

        # Fine-grained: sentence level
        gt_chunks = chunk_text_into_sentences(ground_truth_text)
        pred_chunks = chunk_text_into_sentences(prediction_text)

        precision, recall, f1 = self.compute_fine_grained_similarity(
            gt_chunks, pred_chunks
        )

        # Harmonic mean
        if cosine_sim > 0 and f1 > 0:
            hm_cf = 2 * (cosine_sim * f1) / (cosine_sim + f1)
        else:
            hm_cf = 0.0

        return SimilarityScore(
            cosine_similarity=cosine_sim,
            normalized_cosine=cosine_sim,
            fine_grained_precision=precision,
            fine_grained_recall=recall,
            fine_grained_f1=f1,
            hm_cf=hm_cf,
            metadata={'embedding_model': 'new-model'}
        )

    def compute_fine_grained_similarity(self, gt_chunks, pred_chunks):
        """Compute sentence-level precision, recall, F1."""
        if not gt_chunks or not pred_chunks:
            return 0.0, 0.0, 0.0

        gt_embeddings = self.encode_texts(gt_chunks)
        pred_embeddings = self.encode_texts(pred_chunks)

        # Precision
        precision_scores = []
        for pred_emb in pred_embeddings:
            similarities = [float(np.dot(pred_emb, gt_emb))
                          for gt_emb in gt_embeddings]
            precision_scores.append(max(similarities))
        precision = np.mean(precision_scores)

        # Recall
        recall_scores = []
        for gt_emb in gt_embeddings:
            similarities = [float(np.dot(gt_emb, pred_emb))
                          for pred_emb in pred_embeddings]
            recall_scores.append(max(similarities))
        recall = np.mean(recall_scores)

        # F1
        if precision > 0 and recall > 0:
            f1 = 2 * (precision * recall) / (precision + recall)
        else:
            f1 = 0.0

        return precision, recall, f1
```

**4. Create evaluation script**

File: `src/scripts/run_new_model_evaluation.py`
```python
#!/usr/bin/env python3
"""
New Model Evaluation Script
"""
import sys
import argparse
import logging
from pathlib import Path
from datetime import datetime

# Add src directory to path
sys.path.insert(0, str(Path(__file__).parent.parent / "utils"))

from config_loader import load_config
from new_model_model import NewModelEmbedding
from result_manager import SharedResultManager
from base_types import EmbeddingEvaluationResult

def setup_logging(config):
    """Setup logging configuration."""
    embedding_model_name = config['embedding_model']['name']
    results_base_dir = Path(config['data_paths']['results_base_dir'])
    log_dir = results_base_dir / "embedding_models" / "logs" / embedding_model_name
    log_dir.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = log_dir / f"new_model_evaluation_{timestamp}.log"

    logging.basicConfig(
        level=getattr(logging, config.get('logging', {}).get('level', 'INFO')),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler()
        ]
    )

    logger = logging.getLogger('new_model_main')
    logger.info(f"Logging initialized. Log file: {log_file}")
    return logger

def run_evaluation(config, target_models=None):
    """Run evaluation using New Model."""
    logger = logging.getLogger('new_model_evaluation')

    # Load ground truth
    logger.info("Loading ground truth data...")
    ground_truth_file = Path(config['data_paths']['ground_truth_file'])
    import json
    with open(ground_truth_file, 'r') as f:
        ground_truth_data = json.load(f)
    logger.info(f"Loaded {len(ground_truth_data)} ground truth samples")

    # Initialize result manager
    results_base_dir = Path(config['data_paths']['results_base_dir'])
    result_manager = SharedResultManager(results_base_dir, config['embedding_model']['name'])

    # Initialize embedding model
    logger.info("Initializing New Model...")
    model = NewModelEmbedding(
        model_path=config['embedding_model']['path'],
        device=config['processing']['device'],
        batch_size=config['embedding_model']['batch_size'],
        max_length=config['embedding_model']['max_length']
    )

    if not model.load_model():
        logger.error("Failed to load model")
        return {'success': False}

    logger.info("Model loaded successfully")

    # Get models to evaluate
    models_to_evaluate = target_models or config.get('models_to_evaluate', [])
    predictions_dir = Path(config['data_paths']['predictions_dir'])

    logger.info(f"Starting evaluation for {len(models_to_evaluate)} models")

    for model_idx, model_name in enumerate(models_to_evaluate, 1):
        logger.info(f"[{model_idx}/{len(models_to_evaluate)}] Processing: {model_name}")

        # Load predictions
        pred_file = predictions_dir / f"{model_name}.json"
        if not pred_file.exists():
            logger.error(f"Predictions file not found: {pred_file}")
            continue

        with open(pred_file, 'r') as f:
            predictions = json.load(f)

        logger.info(f"Loaded {len(predictions)} predictions")

        # Process each video
        for gt_item in ground_truth_data:
            video_id = gt_item['id']
            ground_truth_text = gt_item['summary']

            if video_id not in predictions:
                logger.warning(f"No prediction for video {video_id}")
                continue

            prediction_text = predictions[video_id]

            # Compute similarity
            similarity_score = model.compute_similarity(ground_truth_text, prediction_text)

            # Create result
            result = EmbeddingEvaluationResult(
                video_id=video_id,
                model_name=model_name,
                ground_truth_text=ground_truth_text,
                prediction_text=prediction_text,
                embedding_model_scores={config['embedding_model']['name']: similarity_score},
                success=True,
                timestamp=datetime.now().isoformat()
            )

            # Save result
            result_manager.save_individual_result(result)

        # Update model results
        result_manager.update_model_results(model_name)
        logger.info(f"Completed {model_name}")

    logger.info("Evaluation completed successfully")
    return {'success': True}

def main():
    parser = argparse.ArgumentParser(description='New Model Evaluation')
    parser.add_argument('--config', type=str, default='src/configs/new_model.yaml')
    parser.add_argument('--models', nargs='+', help='Specific models to evaluate')
    args = parser.parse_args()

    config = load_config(args.config)
    logger = setup_logging(config)

    logger.info("Starting New Model evaluation")
    result = run_evaluation(config, args.models)

    if result['success']:
        logger.info("Evaluation completed successfully")
    else:
        logger.error("Evaluation failed")
        sys.exit(1)

if __name__ == '__main__':
    main()
```

**5. Download model weights**

```bash
huggingface-cli download org/new-model-name \
  --local-dir embedding_models/New-Model-Name \
  --local-dir-use-symlinks False
```

**6. Update master script**

Edit `run_all_evaluations.sh` to include the new model:
```bash
# Add after existing evaluations
run_evaluation \
    "New-Model-Name" \
    "${VENV_CONFIGS_DIR}/activate_new_model_env.sh" \
    "${SCRIPT_DIR}/src/scripts/run_new_model_evaluation.py" \
    "${SCRIPT_DIR}/src/configs/new_model.yaml"
```

**7. Test the implementation**

```bash
# Test with one VLM first
source venv_configs/activate_new_model_env.sh
python src/scripts/run_new_model_evaluation.py \
  --config src/configs/new_model.yaml \
  --models internvl
deactivate

# Check results
ls results/embedding_models/aggregated_results/New-Model-Name/

# If successful, run full evaluation
./run_all_evaluations.sh
```

## Key Features

- **Multi-Model Ensemble**: 5 state-of-the-art embedding models provide robust evaluation
- **Multi-Granularity**: Document-level and sentence-level similarity metrics
- **Verified Implementations**: Official model implementations from HuggingFace
- **Comprehensive Coverage**: 17 vision-language models evaluated
- **Reproducible**: Isolated environments per embedding model
- **Production-Ready**: Logging, error handling, GPU memory management
- **Flexible**: Easy to add new VLMs or embedding models

## Citation

If you use CLIP-CC Bench in your research, please cite:

```bibtex
@inproceedings{clip-cc-bench-2025,
  title={CLIP-CC Bench: Evaluating Paragraph-Level Video Descriptions in Video–Language Models},
  author={Your Name and Collaborators},
  booktitle={Conference Name},
  year={2025}
}
```

## License

[Specify your license here - MIT, Apache 2.0, etc.]

## Acknowledgments

- **NVIDIA** for NV-Embed-v2 and llama-embed-nemotron-8b models
- **PGFoundation** for KaLM-Embedding-Gemma3-12B-2511 model
- **Alibaba NLP** for gte-Qwen2-7B-instruct model
- **Qwen Team** for Qwen3-Embedding-8B model
- **HuggingFace** for model hosting and transformers library
- **Sentence-Transformers** for the embedding framework
- **NLTK** for sentence tokenization

## Contact

For questions or issues, please:
- Open an issue on GitHub
- Contact: [your-email@domain.com]

---

**Version**: 1.0
**Last Updated**: November 2025
**Status**: Production
