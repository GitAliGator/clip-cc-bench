# CLIP-CC Bench: Evaluating Paragraph-Level Video Descriptions in Video–Language Models

A comprehensive evaluation framework that uses state-of-the-art text embedding models to measure the quality of video descriptions generated by vision-language models (VLMs). The framework computes semantic similarity between model predictions and reference descriptions at both document-level and sentence-level granularity.

## Overview

CLIP-CC Bench is a text embedding-based evaluation framework that:
- Evaluates 17 vision-language models (VLMs) using an ensemble of 5 state-of-the-art text embedding models
- Ranks VLMs based on aggregated performance across all embedding model judges


## Project Structure

```
clip-cc-bench/
├── src/
│   ├── configs/                      # Embedding model configurations (YAML)
│   │   ├── nv-embed.yaml            # NV-Embed-v2 configuration
│   │   ├── kalm.yaml                # KaLM-Embedding-Gemma3-12B-2511
│   │   ├── nemo.yaml                # llama-embed-nemotron-8b
│   │   ├── gte.yaml                 # gte-Qwen2-7B-instruct
│   │   └── qwen.yaml                # Qwen3-Embedding-8B
│   ├── scripts/                      # Evaluation and ranking scripts
│   │   ├── run_nv_embed_evaluation.py
│   │   ├── run_kalm_evaluation.py
│   │   ├── run_nemo_evaluation.py
│   │   ├── run_gte_evaluation.py
│   │   ├── run_qwen_evaluation.py
│   │   └── rank_vlms.py             # VLM ranking algorithm
│   └── utils/                        # Model implementations and utilities
│       ├── nv_embed_model.py        # Model-specific implementations
│       ├── kalm_model.py
│       ├── nemo_model.py
│       ├── gte_model.py
│       ├── qwen_model.py
│       ├── result_manager.py        # Results aggregation system
│       ├── base_types.py            # Shared data structures
│       ├── text_chunking.py         # Sentence tokenization
│       └── paths.py                 # Path management
├── embedding_models/                 # Model weights directory
│   ├── NV-Embed-v2/                 # See embedding_models/README.md
│   ├── KaLM-Embedding-Gemma3-12B-2511/
│   ├── llama-embed-nemotron-8b/
│   ├── gte-Qwen2-7B-instruct/
│   └── Qwen3-Embedding-8B/
├── venv_configs/                     # Virtual environment activation scripts
│   ├── activate_nv-embed_env.sh     # See venv_configs/README.md
│   ├── activate_kalm_env.sh
│   ├── activate_nemo_env.sh
│   ├── activate_gte_env.sh
│   └── activate_qwen_env.sh
├── data/
│   ├── ground_truth/
│   │   └── clip_cc_dataset.json     # Reference descriptions
│   └── predictions/                  # VLM-generated descriptions
│       ├── internvl.json
│       ├── llava_next_video.json
│       └── ... (17 VLM models)
├── results/                          # Evaluation outputs
│   ├── embedding_models/
│   │   ├── logs/                    # Per-embedding model execution logs
│   │   ├── aggregated_results/      # Summary statistics per VLM
│   │   ├── individual_results/      # Per-video, per-VLM detailed results
│   │   │   ├── json/
│   │   │   └── csv/
│   │   └── aggregated_results.csv   # Cross-model summary table
│   └── ranking/                     # VLM ranking results
│       ├── vlm_overall_ranking.csv
│       └── vlm_per_judge_metrics.csv
├── run_all_evaluations.sh           # Master evaluation script
└── README.md
```

## Embedding Models

The framework uses an ensemble of 5 state-of-the-art text embedding models as judges:

| Embedding Model | Parameters | Embedding Dim | Max Length | Implementation | Status |
|-----------------|-----------|---------------|------------|----------------|---------|
| **NV-Embed-v2** (NVIDIA) | - | 4,096 | 32,768 | Custom local model | ✅ Verified |
| **KaLM-Embedding-Gemma3-12B-2511** (PGFoundation) | 12B | - | - | SentenceTransformer | ✅ Verified |
| **llama-embed-nemotron-8b** (NVIDIA) | 8B | 4,096 | 4,096 | SentenceTransformer | ✅ Verified |
| **gte-Qwen2-7B-instruct** (Alibaba) | 7B | 3,584 | 32,768 | SentenceTransformer | ✅ Verified |
| **Qwen3-Embedding-8B** (Qwen) | 8B | Variable | 32,768 | SentenceTransformer | ✅ Verified |

**Implementation Verification**: All models use official/recommended implementations from their respective HuggingFace model cards, ensuring consistency with published results.

## Video Language Models Evaluated

The framework evaluates predictions from **17 vision-language models**:

- **InternVL** (internvl)
- **LLaVA-NeXT-Video** (llava_next_video)
- **LLaVA-OneVision** (llava_one_vision)
- **LongVA** (longva)
- **LongVU** (longvu)
- **MiniCPM** (minicpm)
- **mPLUG** (mplug)
- **Oryx** (oryx)
- **Qwen2.5-32B** (Qwen2.5-32B)
- **Qwen2.5-72B** (Qwen2.5-72B)
- **ShareGPT4** (sharegpt4)
- **TimeChat** (timechat)
- **TS-LLaVA** (ts_llava)
- **Video-XL** (video_xl)
- **VideoChatFlash** (videochatflash)
- **VideoLLaMA3** (videollama3)
- **ViLaMP** (vilamp)

## Setup

### Prerequisites

- Python 3.9+
- CUDA-capable GPU (24GB+ VRAM recommended for large embedding models)
- ~70GB disk space for all embedding models
- PyTorch 2.0+
- transformers >= 4.42.0
- sentence-transformers >= 2.7.0

### Installation

**1. Clone the repository**
```bash
git clone https://github.com/YOUR_USERNAME/clip-cc-bench.git
cd clip-cc-bench
```

**2. Setup virtual environments**

Each embedding model requires an isolated virtual environment with specific dependencies. See `venv_configs/README.md` for detailed setup instructions:

```bash
# Setup all 5 virtual environments
# Follow instructions in venv_configs/README.md
```

Quick activation reference:
```bash
# Activate specific environment
source venv_configs/activate_nv-embed_env.sh
source venv_configs/activate_kalm_env.sh
source venv_configs/activate_nemo_env.sh
source venv_configs/activate_gte_env.sh
source venv_configs/activate_qwen_env.sh

# Deactivate when done
deactivate
```

**3. Download embedding models**

Download model weights from HuggingFace to the `embedding_models/` directory. See `embedding_models/README.md` for complete download instructions and storage requirements (~70GB total).

**4. Prepare evaluation data**

Organize your data files:
```bash
# Reference descriptions (ground truth)
data/ground_truth/clip_cc_dataset.json

# VLM-generated predictions
data/predictions/{vlm_name}.json
```

## Reproducing Results

### Option 1: Run All Evaluations (Recommended)

The master script runs all 5 embedding model evaluations sequentially and computes final VLM rankings:

```bash
./run_all_evaluations.sh
```

This script will:
1. Activate each embedding model's virtual environment
2. Run evaluation for all 17 VLMs using that embedding model
3. Generate individual results, aggregated statistics, and logs
4. Move to the next embedding model
5. Compute final VLM rankings across all embedding models
6. Save comprehensive results to `results/` directory

**Output locations:**
- Logs: `results/embedding_models/logs/{embedding_model_name}/`
- Individual results: `results/embedding_models/individual_results/json/{embedding_model_name}/`
- Aggregated statistics: `results/embedding_models/aggregated_results/{embedding_model_name}/`
- Cross-model summary: `results/embedding_models/aggregated_results/aggregated_results.csv`
- VLM rankings: `results/ranking/vlm_overall_ranking.csv`

### Option 2: Run Individual Embedding Models

Evaluate using a specific embedding model:

```bash
# Activate the target environment
source venv_configs/activate_nv-embed_env.sh

# Run evaluation with config file
python src/scripts/run_nv_embed_evaluation.py --config src/configs/nv-embed.yaml

# Deactivate when done
deactivate
```

Repeat for other embedding models:
```bash
# KaLM
source venv_configs/activate_kalm_env.sh
python src/scripts/run_kalm_evaluation.py --config src/configs/kalm.yaml
deactivate

# Nemotron
source venv_configs/activate_nemo_env.sh
python src/scripts/run_nemo_evaluation.py --config src/configs/nemo.yaml
deactivate

# GTE
source venv_configs/activate_gte_env.sh
python src/scripts/run_gte_evaluation.py --config src/configs/gte.yaml
deactivate

# Qwen3
source venv_configs/activate_qwen_env.sh
python src/scripts/run_qwen_evaluation.py --config src/configs/qwen.yaml
deactivate
```

### Option 3: Evaluate Specific VLMs Only

To evaluate a subset of VLMs (useful for testing):

```bash
source venv_configs/activate_nv-embed_env.sh

python src/scripts/run_nv_embed_evaluation.py \
  --config src/configs/nv-embed.yaml \
  --models internvl llava_one_vision longvu

deactivate
```

### Generate VLM Rankings

After running all embedding model evaluations, generate the final rankings:

```bash
python src/scripts/rank_vlms.py
```

This creates:
- `results/ranking/vlm_overall_ranking.csv` - Final VLM rankings with mean scores across all judges
- `results/ranking/vlm_per_judge_metrics.csv` - Detailed per-embedding-model metrics

## Understanding Results

### Result Files Structure

```
results/embedding_models/
├── logs/
│   ├── NV-Embed-v2/
│   │   └── nv_embed_evaluation_YYYYMMDD_HHMMSS.log
│   ├── KaLM-Embedding-Gemma3-12B-2511/
│   ├── llama-embed-nemotron-8b/
│   ├── gte-Qwen2-7B-instruct/
│   └── Qwen3-Embedding-8B/
├── aggregated_results/
│   ├── NV-Embed-v2/
│   │   ├── internvl.json
│   │   ├── llava_next_video.json
│   │   └── ... (one per VLM)
│   ├── KaLM-Embedding-Gemma3-12B-2511/
│   ├── ... (per embedding model)
│   └── aggregated_results.csv        # Cross-model summary
├── individual_results/
│   ├── json/
│   │   ├── NV-Embed-v2/
│   │   │   ├── internvl.json         # All videos for this VLM
│   │   │   └── ...
│   │   └── ... (per embedding model)
│   └── csv/
│       └── ... (CSV versions)

results/ranking/
├── vlm_overall_ranking.csv           # Final rankings
└── vlm_per_judge_metrics.csv         # Per-judge details
```



## Data Format

### Reference Descriptions (Ground Truth)

File: `data/ground_truth/clip_cc_dataset.json`

```json
[
  {
    "id": "001",
    "summary": "The video begins with a man wearing sunglasses and a light-colored shirt driving a car. He appears to be in a tense situation, gripping the steering wheel tightly. The scene transitions to a close-up of the same man, now with a determined expression, still holding the steering wheel..."
  },
  {
    "id": "002",
    "summary": "The video begins with a close-up of a person lying down with their eyes closed, wearing headphones..."
  }
]
```


### VLM Predictions

File: `data/predictions/{vlm_name}.json`

```json
{
  "001": "The video shows a man driving a car while appearing tense. He grips the steering wheel firmly and displays a determined expression throughout the scene...",
  "002": "A person is shown lying down with headphones on, eyes closed, suggesting they are listening to something..."
}
```

## Adding New Models

### Adding a New VLM for Evaluation

To evaluate a VLM not currently in the benchmark:

1. **Generate predictions** in the required format:
   ```json
   {
     "001": "Your VLM's predicted description for video 001...",
     "002": "Your VLM's predicted description for video 002...",
     ...
   }
   ```

2. **Save predictions** to `data/predictions/your_vlm_name.json`

3. **Update configs** to include your VLM:
   ```bash
   # Edit all 5 config files in src/configs/
   # Add "your_vlm_name" to the models_to_evaluate list
   ```

4. **Run evaluation**:
   ```bash
   ./run_all_evaluations.sh
   ```

   Or evaluate with specific embedding model:
   ```bash
   source venv_configs/activate_nv-embed_env.sh
   python src/scripts/run_nv_embed_evaluation.py \
     --config src/configs/nv-embed.yaml \
     --models your_vlm_name
   deactivate
   ```

5. **Check results** in:
   - `results/embedding_models/aggregated_results/{embedding_model}/your_vlm_name.json`
   - `results/ranking/vlm_overall_ranking.csv`


## Citation

If you use CLIP-CC Bench in your research, please cite:

[Coming Soon]

## License

[Coming Soon]

## Acknowledgments

- **NVIDIA** for NV-Embed-v2 and llama-embed-nemotron-8b models
- **PGFoundation** for KaLM-Embedding-Gemma3-12B-2511 model
- **Alibaba NLP** for gte-Qwen2-7B-instruct model
- **Qwen Team** for Qwen3-Embedding-8B model
- **HuggingFace** for model hosting and transformers library
- **Sentence-Transformers** for the embedding framework
- **NLTK** for sentence tokenization

## Contact

For questions or issues, please:
- Open an issue on GitHub
- Contact: [N/A]

---