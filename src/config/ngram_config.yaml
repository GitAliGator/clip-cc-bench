# N-Gram Metrics Configuration
# Traditional metrics evaluation: BLEU-1, BLEU-4, ROUGE-1, ROUGE-4, ROUGE-L, ROUGE-Lsum, METEOR

ngram_metrics:
  # Metrics to compute
  metrics: ["BLEU-1", "BLEU-4", "ROUGE-1", "ROUGE-4", "ROUGE-L", "ROUGE-Lsum", "METEOR"]
  
  # Output formatting  
  decimal_precision: 2              # 2 decimal places for all metrics
  
  # Progress reporting
  progress_interval: 10             # Report progress every N evaluations

# Processing settings - Sequential for consistency with G-VEval
processing:
  batch_size: 1                     # Single sample processing
  max_workers: 1                    # Sequential processing
  checkpoint_interval: 10           # Progress checkpoints
  save_intermediate_results: true   # Save results as they complete
  
  # Memory optimization
  clear_cache_interval: 50          # Clear memory every N evaluations

# Data paths (absolute paths for clip-cc-bench directory)
data_paths:
  ground_truth_file: "/mmfs2/home/jacks.local/mali9292/aaai_student_abstract/clip-cc-bench/data/ground_truth/clip_cc_dataset.json"
  predictions_dir: "/mmfs2/home/jacks.local/mali9292/aaai_student_abstract/clip-cc-bench/data/models"
  results_base_dir: "/mmfs2/home/jacks.local/mali9292/aaai_student_abstract/clip-cc-bench/results"

# Models to evaluate (test subset - only models with available data)
models_to_evaluate:
  - "internvl"
  - "llava_next_video"
  - "llava_one_vision" 
  - "longva"
  - "longvu"
  - "minicpm"
  - "mplug"
  - "oryx"
  - "sharegpt4"
  - "timechat"
  - "ts_llava"
  - "videochatflash"
  - "videollama3"
  - "video_xl"
  - "vilamp"

# Logging configuration
logging:
  level: "INFO"
  log_dir: "results/n-gram/logs"
  log_prefix: "ngram_evaluation"

# NLTK settings
nltk:
  # Thread-safe NLTK resource management
  download_required_data: true
  required_data: ["punkt", "wordnet", "averaged_perceptron_tagger"]