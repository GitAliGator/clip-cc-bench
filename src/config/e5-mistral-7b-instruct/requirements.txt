# E5-Mistral-7B-Instruct Requirements (memory-optimized for 7B model)
# Based on official intfloat/e5-mistral-7b-instruct specifications and sentence-transformers approach

# Core dependencies - using stable versions for 7B model support
torch==2.4.0
transformers==4.56.1
tokenizers==0.19.1

# Primary approach - sentence-transformers (simpler than FlagEmbedding for E5-Mistral)
sentence-transformers>=3.0.0

# Essential utilities
numpy>=1.21.0
PyYAML>=6.0
tqdm
pandas
psutil

# HuggingFace integration
huggingface-hub>=0.21.0
safetensors>=0.4.3

# Memory and performance optimizations for 7B model
accelerate>=0.20.0  # For better GPU memory management
bitsandbytes>=0.41.0  # For potential quantization support
optimum>=1.16.0  # For model optimization

# Text processing and mathematical operations
scikit-learn>=1.3.0
scipy>=1.10.0

# Model-specific requirements
datasets>=2.14.0  # For data handling

# Memory monitoring and optimization
pynvml>=11.4.0  # For GPU memory monitoring
gpustat>=1.1.0  # For GPU utilization tracking

# E5-Mistral-7B-Instruct Model Specifications:
# - Model size: 7B parameters (32 layers)
# - Embedding dimension: 4096
# - Max sequence length: 4096 tokens
# - Requires instruction-based prompting for queries
# - Memory requirements: ~14GB GPU memory (float16), ~28GB (float32)
# - Recommended batch sizes: 2-4 for 24GB GPU, 1-2 for 16GB GPU
# - Based on Mistral-7B-v0.1 architecture
# - Supports multilingual tasks but optimized for English